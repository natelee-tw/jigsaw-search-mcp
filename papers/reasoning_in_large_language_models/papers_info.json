{
  "2405.06707v1": {
    "title": "Hypothesis Testing Prompting Improves Deductive Reasoning in Large Language Models",
    "authors": [
      "Yitian Li",
      "Jidong Tian",
      "Hao He",
      "Yaohui Jin"
    ],
    "summary": "Combining different forms of prompts with pre-trained large language models\nhas yielded remarkable results on reasoning tasks (e.g. Chain-of-Thought\nprompting). However, along with testing on more complex reasoning, these\nmethods also expose problems such as invalid reasoning and fictional reasoning\npaths. In this paper, we develop \\textit{Hypothesis Testing Prompting}, which\nadds conclusion assumptions, backward reasoning, and fact verification during\nintermediate reasoning steps. \\textit{Hypothesis Testing prompting} involves\nmultiple assumptions and reverses validation of conclusions leading to its\nunique correct answer. Experiments on two challenging deductive reasoning\ndatasets ProofWriter and RuleTaker show that hypothesis testing prompting not\nonly significantly improves the effect, but also generates a more reasonable\nand standardized reasoning process.",
    "pdf_url": "http://arxiv.org/pdf/2405.06707v1",
    "published": "2024-05-09"
  },
  "2412.15797v1": {
    "title": "Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning",
    "authors": [
      "Sungjin Park",
      "Xiao Liu",
      "Yeyun Gong",
      "Edward Choi"
    ],
    "summary": "Despite recent advances in large language models, open-source models often\nstruggle to consistently perform well on complex reasoning tasks. Existing\nensemble methods, whether applied at the token or output levels, fail to\naddress these challenges. In response, we present Language model Ensemble with\nMonte Carlo Tree Search (LE-MCTS), a novel framework for process-level\nensembling of language models. LE-MCTS formulates step-by-step reasoning with\nan ensemble of language models as a Markov decision process. In this framework,\nstates represent intermediate reasoning paths, while actions consist of\ngenerating the next reasoning step using one of the language models selected\nfrom a predefined pool. Guided by a process-based reward model, LE-MCTS\nperforms a tree search over the reasoning steps generated by different language\nmodels, identifying the most accurate reasoning chain. Experimental results on\nfive mathematical reasoning benchmarks demonstrate that our approach\noutperforms both single language model decoding algorithms and language model\nensemble methods. Notably, LE-MCTS improves performance by 3.6% and 4.3% on the\nMATH and MQA datasets, respectively, highlighting its effectiveness in solving\ncomplex reasoning problems.",
    "pdf_url": "http://arxiv.org/pdf/2412.15797v1",
    "published": "2024-12-20"
  },
  "2305.13267v1": {
    "title": "Enhance Reasoning Ability of Visual-Language Models via Large Language Models",
    "authors": [
      "Yueting Yang",
      "Xintong Zhang",
      "Wenjuan Han"
    ],
    "summary": "Pre-trained visual language models (VLM) have shown excellent performance in\nimage caption tasks. However, it sometimes shows insufficient reasoning\nability. In contrast, large language models (LLMs) emerge with powerful\nreasoning capabilities. Therefore, we propose a method called TReE, which\ntransfers the reasoning ability of a large language model to a visual language\nmodel in zero-shot scenarios. TReE contains three stages: observation,\nthinking, and re-thinking. Observation stage indicates that VLM obtains the\noverall information of the relative image. Thinking stage combines the image\ninformation and task description as the prompt of the LLM, inference with the\nrationals. Re-Thinking stage learns from rationale and then inference the final\nresult through VLM.",
    "pdf_url": "http://arxiv.org/pdf/2305.13267v1",
    "published": "2023-05-22"
  },
  "2503.07604v2": {
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "authors": [
      "Tianhe Lin",
      "Jian Xie",
      "Siyu Yuan",
      "Deqing Yang"
    ],
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
    "pdf_url": "http://arxiv.org/pdf/2503.07604v2",
    "published": "2025-03-10"
  },
  "2505.17407v1": {
    "title": "Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?",
    "authors": [
      "Zhi Rui Tam",
      "Cheng-Kuang Wu",
      "Yu Ying Chiu",
      "Chieh-Yen Lin",
      "Yun-Nung Chen",
      "Hung-yi Lee"
    ],
    "summary": "Large reasoning models (LRMs) have demonstrated impressive performance across\na range of reasoning tasks, yet little is known about their internal reasoning\nprocesses in multilingual settings. We begin with a critical question: {\\it In\nwhich language do these models reason when solving problems presented in\ndifferent languages?} Our findings reveal that, despite multilingual training,\nLRMs tend to default to reasoning in high-resource languages (e.g., English) at\ntest time, regardless of the input language. When constrained to reason in the\nsame language as the input, model performance declines, especially for\nlow-resource languages. In contrast, reasoning in high-resource languages\ngenerally preserves performance. We conduct extensive evaluations across\nreasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks\n(CulturalBench, LMSYS-toxic), showing that the effect of language choice varies\nby task type: input-language reasoning degrades performance on reasoning tasks\nbut benefits cultural tasks, while safety evaluations exhibit language-specific\nbehavior. By exposing these linguistic biases in LRMs, our work highlights a\ncritical step toward developing more equitable models that serve users across\ndiverse linguistic backgrounds.",
    "pdf_url": "http://arxiv.org/pdf/2505.17407v1",
    "published": "2025-05-23"
  }
}